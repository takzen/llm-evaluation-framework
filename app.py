# app.py
import streamlit as st
import google.generativeai as genai
import os
import pandas as pd
from dotenv import load_dotenv

# --- Page Configuration ---
st.set_page_config(
    page_title="LLM Evaluation System",
    layout="wide",
    initial_sidebar_state="expanded"
)

# --- Load API Key and Configure Gemini ---
load_dotenv()
try:
    genai.configure(api_key=os.getenv("GOOGLE_API_KEY"))
except Exception as e:
    st.error(f"Error configuring Google AI: {e}")
    st.info("Please make sure you have a .env file with your GOOGLE_API_KEY.")
    st.stop()

# --- Helper Functions ---
@st.cache_data
def generate_test_set(context: str, num_questions: int):
    """
    Uses a powerful LLM to generate a test set (questions and ground truth answers)
    based on a given context.
    """
    model = genai.GenerativeModel('gemini-2.5-pro')
    
    prompt = f"""
    Based on the following context, please generate a diverse set of {num_questions} questions.
    For each question, also provide a concise, factual "ground truth" answer based SOLELY on the provided context.
    
    The output should be a list of Python dictionaries, where each dictionary has two keys: "question" and "answer".
    Provide ONLY the Python list of dictionaries, with no other text or explanation.

    CONTEXT:
    ---
    {context}
    ---
    """
    
    try:
        response = model.generate_content(prompt)
        # Clean up the response to get only the list
        cleaned_response = response.text.strip().replace("```python", "").replace("```", "")
        test_set = eval(cleaned_response) # Use eval to parse the string into a list of dicts
        
        # Validate the structure
        if isinstance(test_set, list) and all(isinstance(d, dict) and 'question' in d and 'answer' in d for d in test_set):
            return test_set
        else:
            st.error("The generated test set has an invalid format. Please try again.")
            return None
            
    except Exception as e:
        st.error(f"An error occurred while generating the test set: {e}")
        return None

# --- Streamlit App ---

st.title("⚖️ LLM Evaluation System")
st.write("A tool to evaluate and test the quality of Language Model prompts and responses.")

st.sidebar.header("Configuration")
context_input = st.sidebar.text_area(
    "1. Provide Context",
    height=300,
    placeholder="Paste a long piece of text here (e.g., product documentation, a chapter from a book, a news article...)",
    help="The LLM will generate questions and answers based on this context."
)

num_questions_input = st.sidebar.slider(
    "2. Number of Questions to Generate",
    min_value=3,
    max_value=20,
    value=5
)

if st.sidebar.button("Generate Test Set", type="primary"):
    if context_input:
        with st.spinner("Generating test questions and answers using Gemini 1.5 Pro..."):
            generated_eval_set = generate_test_set(context_input, num_questions_input)
            
            if generated_eval_set:
                st.session_state.eval_set = generated_eval_set
    else:
        st.sidebar.warning("Please provide a context first.")

# --- Display Generated Test Set ---
if 'eval_set' in st.session_state:
    st.header("Generated Evaluation Set")
    st.write("These questions and 'ground truth' answers were generated by Gemini 1.5 Pro based on your context.")
    
    df_eval = pd.DataFrame(st.session_state.eval_set)
    st.dataframe(df_eval)